%
% File acl2016.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{relsize}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


%% Greit å vite:

%% Siteringer:
%% Med parentes rundt hele: \cite{Gusfield:97}
%% Eller med parentes kun rundt år:  \newcite{Gusfield:97}
%% Flere på en gang: \cite{Gusfield:97,Aho:72}


%% Fra instruksjonene, om \aclfinalcopy :

%% By uncommenting {\small\verb|\aclfinalcopy|} at the top of this document, it
%% will compile to produce an example of the camera-ready formatting; by leaving
%% it commented out, the document will be anonymized for initial submission.  When
%% you first create your submission on softconf, please fill in your submitted
%% paper ID where {\small\verb|***|} appears in the
%% {\small\verb|\def\aclpaperid{***}|} definition at the top.

%% The review process is double-blind, so do not include any author information
%% (names, addresses) when submitting a paper for review.  However, you should
%% maintain space for names and addresses so that they will fit in the final
%% (accepted) version.  The ACL 2016 \LaTeX\ style will create a titlebox space of
%% 2.5in for you when {\small\verb|\aclfinalcopy|} is commented out.

%% \subsection{The Ruler}
%% The ACL 2016 style defines a printed ruler which should be presented in the
%% version submitted for review.  The ruler is provided in order that
%% reviewers may comment on particular lines in the paper without
%% circumlocution.  If you are preparing a document without the provided
%% style files, please arrange for an equivalent ruler to
%% appear on the final output pages.  The presence or absence of the ruler
%% should not change the appearance of any other content on the page.  The
%% camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment
%% the {\small\verb|\aclfinalcopy|} command in the document preamble.)


\title{Optimizing a PoS Tag Set for Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  {\tt email@domain}  \\\And
  Third Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Abstraction ensues.
\end{abstract}

%\section{Credits}

\section{Introduction}
\label{sec:intro}
PoS tagging is an important preprocessing step for many NLP tasks,
such as dependency parsing \cite{Niv:Hal:Kub:07,Haj:Cia:Joh:09}, named
entity recognition \cite{Tjo:DeM:03} and sentiment analysis
\cite{Wil:Wie:Hof:09}. Whereas much effort has gone into the
development of PoS-taggers, to the effect that this task is often
considered more or less a solved task, considerably less effort has
been devoted to the empirical evaluation of the PoS tag sets
themselves. Error analysis of PoS-taggers indicate that, whereas
tagger improvement though means of learning algorithm or feature
engineering seems to have reached something of a plateau, linguistic
assessment of the distinctions made by the PoS tag set may be a venue
worth investigating further \cite{Man:11}.  Clearly, the utility of a
PoS tag set is tightly coupled with the the downstream task for which
it is performed. Even so, PoS tag sets are usually employed in a ``one
size fits all'' fashion, regardless of the requirements posed by the
task which makes use of this information.

It is well known that syntactic parsing often benefits from quite
fine-grained morphological distinctions
\cite{Zha:Niv:11,See:Kuh:13}. Morphology interacts with syntax though
phenomena such as agreement and case marking and incorporating
information on morphological properties of words can therefore often
improved parsing performance. However, in a realistic setting where
the aim is to automatically parse raw text, the generation of
morphological information will often require a separate step of
morphological analysis that can be quite costly.

In this paper, we optimize a Norwegian PoS tag set for the task of
data-driven dependency parsing.  We show that the introduction of
morphological distinctions not present in the original tag set, whilst
compromising tagger accuracy, actually leads to significantly improved
parsing accuracy.  In a set of experiments, various morphological
distinctions are introduced and evaluated both intrinsically, i.e. in
terms of PoS-tagging accuracy, and extrinsically, in terms of parsing
accuracy.  Our results show that the introduction of morphological
distinctions not present in the original tag set, whilst compromising
tagger accuracy, actually leads to significantly improved parsing
accuracy. This optimization allows us to bypass the additional step of
morphological analysis, framing the whole preprocessing problem as a
simple tagging task.

The article is structured as follows. We start out by reviewing
previous work on tag set evaluation in Section \ref{sec:prev} and
Section \ref{sec:data} provides an overview of the treebank used for
experimentation. Section \ref{sec:setup} describes the experimental
setup used in our work and Section \ref{sec:optimization} goes on to
provide the results from our optimization experiments. Finally,
Section \ref{sec:summary} summarizes our main findings and discusses
some avenues for future work.

\section{Previous/Related work}
\label{sec:prev}
There has been little previous work dedicated specifically to
extrinsic evaluation of the effects of PoS tag sets on downstream
applications.  There has, however, been some work that evaluates the
effects of PoS tag set granularity on PoS tagging. This includes
investigation of the effects of PoS tag sets on tagging of Swedish
\cite{Meg:01,Meg:02} and English \cite{Mac:05}.

%Increases in tagger accuracy does not readily translate to
%improved parsing.

\cite{Meg:01,Meg:02} trained and evaluated a range of PoS taggers on the
Stockholm-Umeå Corpus (SUC) \cite{Gus:Har:06}, annotated with a tag set based
on a Swedish version of PAROLE tags totaling 139 tags. Furthermore, they
investigated the effects of tag set size on tagging by mapping the original tag
set into smaller subsets designed for parsing. They argue that a tag set with
complete morphological tags may not be necessary for all NLP applications, for
instance syntactic parsing. They found that the smallest tag set comprising 26
tags yields the lowest tagger error rate. However, for some of the taggers,
augmenting the tag set with more linguistically informative tags may actually
lead to a drop in error rate. They argue that this shows that the size of the
tag set as well as the type of information in the tags are crucial factors for
tagger performance. However/unfortunately, they do not report results of
parsing with the various PoS tag sets.

Similarly, \newcite{Mac:05} investigated the effects of PoS tag sets on tagger
performance in English, specifically the Wall Street Journal portion of Penn
Treebank \cite{Mar:San:Mar:93}. They mapped the original tag set of Penn
Treebank to more fine-grained tag sets using linguistic insight to investigate
whether additional linguistic information included in finer-grained tags could
assist the tagger. Experimenting with both lexically and syntactically
conditioned modifications such as distinguishing between count nouns and
noncount nouns and between transitive and intransitive verbs, they found that
more fine-grained tag sets rarely led to improvements in tagger accuracy; the
most successful modification yielded an improvement in tagger accuracy of 0.05
percentage points.

Transition/overgang her ...

\section{Data}
\label{sec:data}
\subsection{The Norwegian Dependency Treebank}
We used the newly developed Norwegian Dependency Treebank (NDT)
\cite{Sol:Skj:Ovr:14},  the first publicly available treebank for Norwegian. It
was developed at the National Library of Norway in collaboration with the
University of Oslo, and contains manual syntactic and morphological annotation.
The treebank contains data from both varieties of Norwegian; 311 000 tokens of
Bokmål and 303 000 tokens of Nynorsk. The annotated texts are mostly newspaper
text, but also include government reports, parliament transcripts and excerpts
from blogs. The annotation process of the treebank was supported by the
Oslo-Bergen Tagger \cite{Hag:Joh:Nok:00} and then manually corrected by
annotators.

%Linguistic motivation, insight, utility...

\paragraph{Morphological Annotation}
The morphological annotation and PoS tag set of NDT follows the Oslo-Bergen
Tagger \cite{Hag:Joh:Nok:00,Sol:13}, which in turn is largely based on the work
of \newcite{Faa:Lie:Van:97}. The tag set consists of 12 morphosyntactic PoS
tags, with 7 additional tags for punctuation and symbols.  The tag set is thus
rather coarse-grained, with broad categories such as \texttt{subst} (noun) and
\texttt{verb} (verb). The PoS tags are complemented by a large set of
morphological features, providing information about morphological properties such as
definiteness, number and tense. These features are used in our tag set
modifications, where the coarse PoS tag of relevant tokens are concatenated
with one or more of these features to include more linguistic information in
the tags.

\begin{table}
    \centering
    %\smaller[0.5]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Tag} & \textbf{Description} \\
        \midrule
        \texttt{adj} & Adjective \\
        \texttt{adv} & Adverb \\
        \texttt{det} & Determiner\\
        \texttt{inf-merke} & Infinitive marker \\
        \texttt{interj} & Interjection \\
        \texttt{konj} & Conjunction \\
        \texttt{prep} & Preposition \\
        \texttt{pron} & Pronoun \\
        \texttt{sbu} & Subordinate conjunction \\
        \texttt{subst} & Noun \\
        \texttt{ukjent} & Unknown (foreign word) \\
        \texttt{verb} & Verb \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the PoS tag set of NDT.}
    \label{ndttagest}
\end{table}

\paragraph{Syntactic Annotation}
The syntactic annotation choices in NDT are largely based on the Norwegian
Reference Grammar \cite{Faa:Lie:Van:97}. The annotation choices are outlined in
Table \ref{ndtannotation}, taken from/courtesy of \newcite{Sol:Skj:Ovr:14},
providing overview of the analyses of syntactic constructions that often
distinguish dependency treebanks, such as coordination and the treatment of
auxiliary and main verbs. The set of dependency relations comprises 29
dependency relations, including \textsc{adv} (adverbial), \textsc{subj}
(subject) and \textsc{koord} (coordination).

\begin{table} \centering
    %\smaller[0.5]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Head} & \textbf{Dependent} \\
        \midrule
        Preposition & Prepositional complement \\
        Finite verb & Complementizer \\
        First conjunct & Subsequent conjuncts \\
        Finite auxiliary & Lexical/main verb \\
        Noun & Determiner \\
        \bottomrule
    \end{tabular}
    \caption{Annotation choices in NDT.}
    \label{ndtannotation}
\end{table}

\section{Experimental Setup}
\label{sec:setup}
In preparation to conducting our experiments with linguistically motivated tag
set modifications, a concrete setup for the experiments needed to be
established, which is presented in the following.

\paragraph{Data Set Split}
As there was no standardized data set split of NDT due to its very recent
development, we needed to establish a data set split
(training/development/test). Our data set split of the treebank follows the
standard 80-10-10 (training/development/test) split and will be distributed
with the treebank and proposed as the new standard(?). In creating the data set
split, care has been taken to preserve contiguous texts in the various data
sets while keeping the split balanced in terms of genre (and source). Our
proposed data set split was used in the Norwegian contribution to the Universal
Dependencies project \cite{Ovr:Hoh:16}.  The split will be made available at a
companion website.

\paragraph{Tagger}
For our experiments with tag set modifications, we wanted a PoS tagger both
reasonably fast and accurate. There is often a trade-off between the two
factors/considerations, as the most accurate taggers tend to suffer in terms of
speed due to their complexity. However, a tagger that achieves both close to
state-of-the-art accuracy as well as very high speed is TnT \cite{Bra:00}. The
fact that TnT was used for evaluating the universal tag set
\cite{Pet:Das:McD:12}, served as another good indication of TnT being
appropriate for our task. The sum of these factors led to TnT being the tagger
of choice for our experiments.

\paragraph{Parser}
In choosing a syntactic parser for our experiments, we considered previous work
on dependency parsing of Norwegian, specifically that of \cite{Sol:Skj:Ovr:14}.
They found the Mate parser \cite{Boh:10} to be the most successful parser for
the parsing of NDT. Furthermore, recent dependency parser comparisons
\cite{Cho:Tet:Ste:15} showed that Mate performed very well on parsing of
English, beating a range of contemporary state-of-the-art parsers.

\paragraph{Tag Set Mapping}
In order to carrying out the tag set modifications, we created a mapping that
maps the relevant existing tags to new, more fine-grained tags including more
relevant morphological features for the applicable tokens. A given tag set
modification involves specifying a tag and one or more features to be
concatenated with said tag for all applicable tokens, i.e., tokens that are
assigned said tag and contains the set of features in its morphological
features.

\paragraph{Baseline}
It is common practice to compare the performance of PoS taggers to a
pre-computed baseline for an initial point of comparison.
For PoS tagging, a commonly used baseline is the Most Frequent Tag (MFT)
baseline, which we use in our experiments. This involves labeling each word
with the tag it was assigned most frequently in the training. All unknown
words, i.e., words not seen in the training data, are assigned the tag most
frequently assigned to words seen only once in the training. Unknown and
infrequent words have in common that they rarely occur, and we might therefore
expect them to have similar properties.
%, there are two main
%approaches: assign it the most frequent tag overall in the training data, or
%the tag most frequently assigned to words seen only once in the training data.
%We adopted the latter approach for our baseline tagger, as we believe that
%unknown words may have much in common with words that occur only once in the
%training, more so than simply words of the most frequent part-of-speech. The
%reason for this is that unknown and infrequent words have in common that they
%rarely occur, and we might therefore expect them to have similar properties.

\paragraph{Tags \& Features}
As we seek to quantify the effects of PoS tagging in a realistic setting, we
want to run the parser on automatically assigned PoS tags. For the training of
the parser, however, we have two options: using either gold standard or
automatically assigned tags. In order to settle on a configuration, we
conducted experiments with gold standard and automatically assigned tags to see
how they differ with respect to performance. The results of our experiments
reveal that the combination of training and testing on automatic tags is
superior to training on gold standard tags and testing on automatic tags.
Consequently, the parser was both trained and tested on automatically assigned
tags in our experiments.

We removed any morphological features in order to simulate a realistic setting.
Moreover, it is crucial that we remove these features when working with
automatically assigned tags, as the features is still gold standard.  For
instance, if a verb token is erroneously tagged as a noun, we could potentially
have a noun token with verbal features such as tense, which markedly obfuscates
the training and parsing. Another important aspect is that we want to isolate
the effect of PoS tags, necessitating the exclusion of morphological features.

%Note that it is absolutely crucial that the morphological features in the
%treebank are removed when using automatic tags, as they are still gold
%standard. For instance, if a verb token is erroneously tagged as a noun, we
%could potentially have a noun token with verbal features such as tense, which
%markedly obfuscates the training and parsing. Another important factor is that
%we want to isolate the effect of PoS tags, necessitating the exclusion of
%morphological features.

\section{Tag Set Optimization/Experiments}
\label{sec:optimization}
With more fine-grained linguistically
motivated distinctions, we increase the linguistic information represented in
the tags, which may assist the tagger in disambiguating ambiguous and unknown
words, which in turn may aid the parser in recognizing and generalizing
syntactic patterns. However, the addition of more linguistic information to the
tags and thus a more fine-grained tag set will most likely lead to a drop in
tagger accuracy, due to the increase in complexity. The best tagging does not
necessarily lead to the best parse, and it is therefore interesting to
investigate how the tag set modifications may affect the interplay between
tagging and parsing.

\subsection{Tag Set Experiments}
We modified the tags for nouns, verbs, adjectives, determiners and pronouns in
NDT by appending selected sets of morphological features to each tag in order
to increase the linguistic information expressed by the tags.  For each tag, we
first experimented with each of the features in isolation before employing
various combinations of them. We based our choices of combinations on how
promising the features are and what we deem worth investigating in terms of
linguistic utility, in order to see how the features might interact.

\paragraph{Nouns}
In Norwegian, nouns are assigned gender (feminine, masculine, neuter),
definiteness (indefinite or definite) and number (singular or plural). There
is agreement in gender, definiteness and number between nouns and their
modifiers (adjectives and determiners).

\paragraph{Verbs}
Verbs are inflected for tense (infinitive, present, preterite, past perfect)
in Norwegian and can additionally take on mood (imperative, indicative) and
voice (active or passive).

\paragraph{Adjectives}
Adjectives agree with the noun they modify in terms of gender, number and
definiteness in Norwegian.

\paragraph{Determiners}
Like adjectives, determiners in Norwegian agree with the noun they modify in
terms of gender, number and definiteness.

\paragraph{Pronouns}
Pronouns in Norwegian include personal, reciprocal, reflexive and
interrogative. They can exhibit gender, number and person. Personal pronouns
have case (accusative or nominative).

%\begin{table}
    %\centering
    %\smaller[0.5]
    %\begin{tabular}{@{}lllll@{}}
        %\toprule
        %\textbf{Tag Set} & \textbf{MFT} & \textbf{Accuracy} &
        %\textbf{LAS} & \textbf{UAS} \\
        %\midrule
        %Original & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        %Full & 85.15\% & 93.48\% & \textbf{87.15\%} & \textbf{90.39\%} \\
        %\bottomrule
    %\end{tabular}
    %\caption{Evaluation of tagging and parsing the development data with the two
        %initial tag sets.}
    %\label{inittagseteval}
%\end{table}

\begin{table*}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}llllll@{}}
        \toprule
        \textbf{Category} & \textbf{Feature(s)} & \textbf{MFT} &
        \textbf{Accuracy} & \textbf{LAS} & \textbf{UAS} \\
        \midrule
        \emph{Baseline} & --- & 94.14\% & 97.47\% & 87.01\% & 90.19\% \\
        Noun & Type, case \& definiteness & 89.61\% & 97.05\% & 88.81\% &
        91.73\% \\
        Verb & Finiteness & 93.72\% & 97.35\% & 87.30\% & 90.43\% \\
        Adjective & Degree & 94.13\% & 97.41\% & 87.29\% & 90.44\% \\
        Determiner & Definiteness & 94.13\% & 97.49\% & 87.30\% & 90.42\% \\
        Pronoun & Type \& case & 94.12\% & 97.51\% & 87.30\% & 90.41\% \\
        \bottomrule
    \end{tabular}
    \caption{Results of tagging and parsing with the most successful tag set
        modification for each category.}
    \label{respectiveresults}
\end{table*}

\begin{table}
    \centering
    \smaller[1]
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Tag} & \textbf{Description} \\
        \midrule
        \texttt{adj|komp} & Comparative adjective \\
        \texttt{adj|pos} & Positive adjective \\
        \texttt{adj|sup} & Superlative adjective \\
        \texttt{det|be} & Definite determiner \\
        \texttt{det|ub} & Indefinite determiner \\
        \texttt{pron|pers} & Personal pronoun \\
        \texttt{pron|pers|akk} & Personal pronoun, accusative \\
        \texttt{pron|pers|nom} & Personal pronoun, nominative \\
        \texttt{pron|refl} & Reflexive pronoun \\
        \texttt{pron|res} & Reciprocal pronoun \\
        \texttt{pron|sp} & Interrogative pronoun \\
        \texttt{subst|appell} & Common noun \\
        \texttt{subst|appell|be} & Common noun, definite \\
        \texttt{subst|appell|be|gen} & Common noun, definite, genitive \\
        \texttt{subst|appell|ub} & Common noun, indefinite \\
        \texttt{subst|appell|ub|gen} & Common noun, indefinite, genitive \\
        \texttt{subst|prop} & Proper noun \\
        \texttt{subst|prop|gen} & Proper noun, genitive \\
        \texttt{verb|fin} & Finite verb \\
        \texttt{verb|infin} & Nonfinite verb \\
        \bottomrule
    \end{tabular}
    \caption{Overview of the optimized tag set.}
    \label{optimizedtagset}
\end{table}

\begin{table}
    \centering
    \smaller[0.5]
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Tag set} & \textbf{MFT} & \textbf{Accuracy} &
        \textbf{LAS} & \textbf{UAS} \\
        \midrule
        Original & \textbf{94.14\%} & \textbf{97.47\%} & 87.01\% & 90.19\% \\
        Full & 85.12\% & 93.46\% & 87.13\% & 90.32\% \\
        Optimized & 89.20\% & 96.85\% & \textbf{88.87\%} & \textbf{91.78\%} \\
        \bottomrule
    \end{tabular}
    \caption{Results of tagging and parsing with the optimized tag set, compared to
        the initial tag sets.}
    \label{finalresults}
\end{table}

\section{Optimized Pipeline}
\label{sec:pipeline}

\section{Summary/Conclusion and Future Work}
\label{sec:summary}

% \section*{Acknowledgments}

\bibliographystyle{acl2016}
\bibliography{references}
% \appendix
%\section{Supplemental Material}
%\label{sec:supplemental}

\end{document}
